# Implementation Plan: RAG Integration for Physical AI Textbook

**Branch**: `002-rag-integration` | **Date**: 2025-12-30 | **Spec**: [spec.md](./spec.md)
**Input**: Feature specification from `specs/002-rag-integration/spec.md`

**Note**: This plan was generated by the `/sp.plan` command following the SDD workflow.

## Summary

**Primary Requirement**: Implement an embedded RAG (Retrieval-Augmented Generation) chatbot within the Docusaurus textbook site that allows students to ask questions about Physical AI content and receive AI-generated answers with source citations.

**Technical Approach**:
- **Backend**: FastAPI service with Qdrant vector database for similarity search and Neon Postgres for session storage
- **LLM**: Gemini API (free tier) via LiteLLM middleware for OpenAI Agents SDK compatibility
- **Embeddings**: Google gemini-embedding-001 (free, 768 dimensions)
- **Chunking**: Markdown-aware hierarchical splitting (512-768 tokens, 10-20% overlap) with chunk_index positioning
- **Frontend**: Custom React chat component integrated into Docusaurus via Root swizzling
- **Caching**: LRU cache (100 entries) + query deduplication (5-minute window) to maximize Gemini's 5 RPM limit
- **Rate Limiting**: Multi-layer strategy (client + backend + caching) supporting 10-15 concurrent users

---

## Technical Context

**Language/Version**: Python 3.10+ (backend), TypeScript 5.6+ (frontend components)

**Primary Dependencies**:
- Backend: FastAPI 0.115+, Qdrant Client 1.7+, LiteLLM 1.50+, Google GenerativeAI SDK 0.8+, slowapi (rate limiting), asyncpg (Neon Postgres client), functools.lru_cache (response caching)
- Frontend: React 19.0, Docusaurus 3.9.2, TypeScript 5.6 (existing from Phase 1)

**Storage**:
- Vector DB: Qdrant Cloud Free Tier (1GB, ~130-150 text chunks)
- Relational DB: Neon Serverless Postgres for chat history and anonymous sessions (Phase 2)
- Caching: In-memory LRU cache (100 entries) for query deduplication (5-minute window)

**Testing**:
- Backend: pytest 8.0+, pytest-asyncio, httpx (for FastAPI testing), pytest-cov
- Frontend: Jest 29+, React Testing Library (existing from Docusaurus)
- Integration: E2E tests with Playwright

**Target Platform**:
- Backend: Linux server (Vercel/Railway/Render serverless functions)
- Frontend: Static site deployment (GitHub Pages with GitHub Actions CI/CD)

**Project Type**: Web application (backend API + frontend integration)

**Performance Goals**:
- RAG query response: p95 <2s, p99 <5s (per Constitution Principle IX)
- Vector search: <100ms for top 10 results
- Embedding generation: <1s per 1000 tokens
- Frontend: FCP <1.5s, TTI <3.5s (per Constitution performance budgets)

**Constraints**:
- **Critical**: Gemini Free Tier: 5 RPM / 100 RPD (most restrictive constraint)
- Qdrant Free Tier: 1GB storage, 100 RPS (adequate for hackathon scale)
- No authentication required for Phase 2 (Constitution Principle VII: core first)
- Must gracefully degrade when backend offline (Constitution Principle IX)
- Must preserve accessibility (keyboard navigation, screen readers)

**Scale/Scope**:
- ~130-150 text chunks (13 weeks × 10 chunks/week average)
- Support 10-15 concurrent users (adjusted for Gemini 5 RPM constraint)
- Anonymous session storage in Neon Postgres (Phase 2)
- English-only responses (Urdu translation is Phase 4 bonus)

---

## Constitution Check

*GATE: Must pass before Phase 0 research. Re-checked after Phase 1 design.*

### ✅ Principle I: AI-Driven Content Creation (Spec-Driven Development)

**Compliance**: This plan follows Constitution → Spec → Plan → Tasks workflow.

**Evidence**:
- Spec exists at `specs/002-rag-integration/spec.md` (created via `/sp.specify`)
- This plan fills the template structure as required
- All decisions documented with rationale in `research.md`

**Status**: **PASS**

---

### ✅ Principle II: Interactive Learning Experience (RAG Integration)

**Compliance**: Implements embedded RAG chatbot with full-text + text-selection queries as mandated.

**Evidence**:
- Single API endpoint `/api/chat/query` with `query_type` enum handles both full-text and text-selection queries
- Chat component integrated into Docusaurus via Root swizzling
- Tech stack uses OpenAI Agents SDK (via LiteLLM), FastAPI, **Neon Postgres**, Qdrant, Gemini as specified in Constitution

**Status**: **PASS** (corrected: Neon now included in Phase 2)

---

### ✅ Principle III: Content Quality & Technical Accuracy

**Compliance**: RAG responses MUST cite sources; no hallucination allowed.

**Evidence**:
- Response schema includes `citations[]` field with source, chunk_id, relevance_score
- System prompt enforces citation format: "[Week X, Section Y]"
- Acceptance criteria in spec verifies 80% of queries return valid citations (SC-002)

**Status**: **PASS**

---

### ✅ Principle IV: Modular Architecture (Separation of Concerns)

**Compliance**: Backend API independently deployable; frontend makes REST calls.

**Evidence**:
- Clear API contracts defined in `contracts/chat-api.md`
- Backend runs on separate server (Vercel/Railway)
- No tight coupling between frontend and backend
- OpenAPI documentation auto-generated at `/api/docs`

**Status**: **PASS**

---

### ✅ Principle V: Accessibility & Personalization

**Compliance**: Chat widget keyboard-navigable; works without JS (static content fallback).

**Evidence**:
- Chat button accessible via Tab key
- Modal closable with Esc key
- Content remains readable if chat fails (graceful degradation)
- Phase 4: Personalization based on user background (out of scope for Phase 2)

**Status**: **PASS**

---

### ✅ Principle VI: Security & Privacy

**Compliance**: No hardcoded API keys; rate limiting implemented; SSL/TLS for DB connections.

**Evidence**:
- `.env` file for secrets (`.env.example` template provided)
- `slowapi` rate limiter configured (3 req/min per IP)
- Qdrant Cloud uses TLS by default
- Security scan gate in deployment checklist (Constitution Section 4)

**Status**: **PASS**

---

### ✅ Principle VII: Progressive Enhancement (Core First, Bonus Layered)

**Compliance**: RAG chatbot is Phase 2 (after Phase 1 foundation); authentication is Phase 4.

**Evidence**:
- Feature flags for future personalization: `FEATURE_AUTH_ENABLED=false` in MVP
- Priority order: Foundation (Phase 1) → RAG (Phase 2) → Content Scale (Phase 3) → Enhancements (Phase 4)
- Completing Core Requirements (100 points) before attempting bonus features

**Status**: **PASS**

---

### ✅ Principle VIII: Documentation-First & Traceability

**Compliance**: All endpoints documented in contracts/; OpenAPI schema auto-generated.

**Evidence**:
- This plan includes API contracts (`contracts/chat-api.md`), data models (`data-model.md`), quickstart guide (`quickstart.md`)
- FastAPI generates OpenAPI schema automatically
- PHR will be created after plan completion (per Constitution workflow)

**Status**: **PASS**

---

### ✅ Principle IX: Graceful Degradation & Reliability

**Compliance**: Frontend displays textbook content even if backend offline.

**Evidence**:
- API returns 503 with fallback message: "AI assistant temporarily offline. Browse content normally."
- Frontend catches errors and displays user-friendly message
- Health check endpoint (`/api/health`) for status verification
- Monitoring requirements defined (99% uptime for static content, 95% for dynamic features)

**Status**: **PASS**

---

### Summary

**All 9 Constitution principles are satisfied. No violations. No complexity justification required.**

---

## Project Structure

### Documentation (this feature)

```text
specs/002-rag-integration/
├── plan.md              # This file (/sp.plan output)
├── research.md          # Phase 0 research findings ✅ Created
├── data-model.md        # Data schemas and storage ✅ Created
├── quickstart.md        # Setup and deployment guide ✅ Created
├── contracts/           # API contracts directory
│   └── chat-api.md      # Chat endpoints specification ✅ Created
└── tasks.md             # Phase 2 output (created by /sp.tasks - NOT YET)
```

---

### Source Code (repository root)

```text
# Backend API (new directory to create)
backend/
├── src/
│   ├── main.py                      # FastAPI app entry point
│   ├── config.py                    # Environment variables, settings
│   ├── models/
│   │   ├── chat.py                  # Pydantic models for requests/responses
│   │   ├── chunk.py                 # TextChunk dataclass
│   │   └── health.py                # HealthCheck models
│   ├── services/
│   │   ├── embedding_service.py     # Google Gemini embeddings
│   │   ├── vector_store.py          # Qdrant client wrapper
│   │   ├── llm_service.py           # LiteLLM + Gemini integration
│   │   └── rag_pipeline.py          # Orchestrates retrieval + generation
│   ├── routers/
│   │   ├── chat.py                  # /api/chat/* endpoints
│   │   ├── health.py                # /api/health endpoint
│   │   └── admin.py                 # /admin/index endpoint
│   └── utils/
│       ├── chunking.py              # Markdown chunking logic
│       ├── rate_limiter.py          # slowapi configuration
│       └── error_handlers.py        # Custom exception handlers
├── scripts/
│   ├── index_docs.py                # CLI tool to index documentation
│   └── setup_qdrant.py              # Create Qdrant collection
├── tests/
│   ├── unit/
│   │   ├── test_chunking.py
│   │   ├── test_embedding.py
│   │   └── test_vector_store.py
│   ├── integration/
│   │   ├── test_rag_pipeline.py
│   │   └── test_api_endpoints.py
│   └── e2e/
│       └── test_chat_flow.py
├── data/
│   └── index_metadata.json          # Indexing state
├── requirements.txt                 # Python dependencies
├── .env.example                     # Template for environment variables
├── Dockerfile                       # For deployment
└── README.md                        # Backend setup instructions

# Frontend Integration (existing + new components)
physical-ai-textbook/
├── src/
│   ├── components/
│   │   ├── RAGChatbot/              # NEW: Chat component
│   │   │   ├── index.tsx            # Main chat widget
│   │   │   ├── ChatModal.tsx        # Modal dialog
│   │   │   ├── MessageList.tsx      # Chat history display
│   │   │   ├── QueryInput.tsx       # User input field
│   │   │   ├── Citation.tsx         # Source citation display
│   │   │   ├── styles.module.css    # Component styles
│   │   │   └── types.ts             # TypeScript interfaces
│   │   └── HomepageFeatures/        # EXISTING
│   ├── hooks/
│   │   ├── useChatAPI.ts            # NEW: API client hook
│   │   ├── useTextSelection.ts      # NEW: Track text selection
│   │   └── useSessionStorage.ts     # NEW: Chat history persistence
│   ├── theme/
│   │   └── Root.tsx                 # SWIZZLED: Add global chat widget
│   └── pages/
│       └── index.tsx                # EXISTING
├── docusaurus.config.ts             # MODIFIED: Add API proxy
└── package.json                     # MODIFIED: Dependencies (if any new ones)

# Deployment Configuration (new)
.github/
└── workflows/
    ├── backend-deploy.yml           # NEW: Deploy backend to Vercel/Railway
    └── frontend-deploy.yml          # EXISTING: GitHub Pages deployment
```

---

**Structure Decision**:

Selected **Option 2: Web application** (backend + frontend) from template.

**Rationale**:
- **Modular architecture** allows independent scaling and deployment (Constitution Principle IV)
- **Backend** can be deployed to any Python-compatible serverless platform (Vercel, Railway, Render)
- **Frontend** remains a static site with progressive enhancement (Constitution Principle VII)
- **Clear separation** enables parallel development of API and UI components
- **Future extensibility** for Phase 4 features (authentication, personalization) without disrupting core

---

## Complexity Tracking

> **No violations detected. This section is empty as all Constitution principles are satisfied.**

---

## Critical Files for Implementation

Based on comprehensive planning, these are the 3-5 most critical files:

1. **`backend/src/services/rag_pipeline.py`**
   - Core RAG orchestration logic combining retrieval from Qdrant and generation with Gemini
   - Implements citation extraction and hallucination prevention
   - **Priority**: P1 (MVP blocker)

2. **`backend/src/routers/chat.py`**
   - FastAPI endpoint handler for single `/api/chat/query` endpoint with query_type discrimination
   - Critical for API contract compliance
   - **Priority**: P1 (MVP blocker)

3. **`backend/scripts/index_docs.py`**
   - Documentation indexing script using MarkdownHeaderTextSplitter
   - Must run before any queries work
   - **Priority**: P1 (MVP blocker)

4. **`physical-ai-textbook/src/components/RAGChatbot/index.tsx`**
   - Main React chat widget component integrated into Docusaurus
   - User-facing interface for the RAG system
   - **Priority**: P1 (MVP blocker)

5. **`backend/src/services/llm_service.py`**
   - LiteLLM integration with Gemini API
   - Handles rate limiting, exponential backoff, LRU caching (100 entries), and response generation with chunk-position citations
   - **Priority**: P1 (MVP blocker)

6. **`backend/src/services/database_service.py`** (NEW)
   - Neon Postgres integration for anonymous session storage
   - Manages query_sessions and session_messages tables
   - **Priority**: P1 (MVP blocker)

---

## Phase 0: Research (Complete) ✅

See `research.md` for comprehensive findings. Key decisions:

1. **OpenAI Agents SDK + Gemini**: Use LiteLLM middleware
2. **Embedding Model**: Google gemini-embedding-001 (free, 768-dim, 1500 RPM)
3. **Chunking**: Markdown-aware hierarchical (512-768 tokens, 10-20% overlap)
4. **Qdrant**: Cosine similarity, indexed payloads for week/module
5. **Rate Limiting**: Multi-layer (client + backend + caching)
6. **Frontend**: Custom React component with swizzled Root

---

## Phase 1: Design & Contracts (Complete) ✅

See artifacts:
- **Data Model**: `data-model.md` - TextChunk, ChatMessage, QuerySession, IndexMetadata
- **API Contracts**: `contracts/chat-api.md` - 4 endpoints with full schemas
- **Quickstart Guide**: `quickstart.md` - Setup instructions for backend + frontend

---

## Next Steps

1. **Run `/sp.tasks`** to break down implementation into testable tasks
2. **Run `/sp.implement`** to execute tasks systematically
3. **Test** with sample queries from spec acceptance scenarios (spec.md:18-22)
4. **Deploy** to staging environment (Vercel backend + GitHub Pages frontend)
5. **Run Constitution compliance checklist** (Constitution Section 4 Deployment Gates)
6. **Deploy to production**
7. **Create PHR** documenting deployment and learnings

---

## Planning Complete

**Status**: ✅ Ready for `/sp.tasks`

**Artifacts Created**:
- ✅ research.md (6 research decisions with sources)
- ✅ data-model.md (5 entity schemas + validation rules)
- ✅ contracts/chat-api.md (4 API endpoints with full OpenAPI specs)
- ✅ quickstart.md (Setup guide with troubleshooting)
- ✅ plan.md (This file - comprehensive implementation plan)

**Constitution Check**: ✅ All 9 principles satisfied, no violations

**Next Command**: `/sp.tasks` to generate actionable implementation tasks
